{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Beam - Colab - LZD",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/logsmay/inter-probe/blob/main/Apache_Beam_Colab_LZD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5uPfKt9guxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "31237398-f6df-4900-d504-ebf77306878e"
      },
      "source": [
        "# Run and print a shell command.\n",
        "def run(cmd):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')\n",
        "\n",
        "# Install apache-beam.\n",
        "run('pip install --quiet apache-beam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> pip install --quiet apache-beam\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 57.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 60.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 54.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 58.1MB 37kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 40.7MB/s \n",
            "\u001b[?25h  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.9 has requirement dill>=0.3.1, but you'll have dill 0.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcVGXEFzKL5D"
      },
      "source": [
        "[Apache Beam](https://beam.apache.org/) is an unified programming SDK to design and execute data processing pipelines (both batch & realtime) with multiple supported runners such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/), [Apache Apex](https://apex.apache.org/), [Apache Gearpump](http://incubator.apache.org/projects/gearpump.html), [Apache Samza](http://samza.apache.org/) and [Google Dataflow](https://cloud.google.com/dataflow/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQhbLINLhLTF"
      },
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBCKpBk_hb3i"
      },
      "source": [
        "logging.getLogger().setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbvIosP2MGnO"
      },
      "source": [
        "Apache Beam supports [multiple variants of runners](https://beam.apache.org/documentation/runners/capability-matrix/) to build and process data pipelines through highly sophisticated parallel processing. Each runners can be configured accordingly with the respective configuration through pipeline options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQygB8H3heVs"
      },
      "source": [
        "# Current option uses direct runner (local python kernel)\n",
        "option_dict = {\n",
        "    'runner': 'DirectRunner',\n",
        "    'job_name': 'notebook',\n",
        "}\n",
        "pipeline_options = PipelineOptions.from_dictionary(option_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEtgerqphgP9"
      },
      "source": [
        "input_data = [\n",
        "    \"1b,20181019,10:00 am,click,ios,SG\",\n",
        "    \"1b,20181019,10:01 am,install,ios,SG\",\n",
        "    \"1c,20181019,10:02 am,page_view,android,SG\",\n",
        "    \"1c,20181019,10:03 am,order,android,SG\",\n",
        "    \"1b,20181019,11:00 am,page_view,ios,SG\",\n",
        "    \"1b,20181019,11:10 am,order,ios,SG\",\n",
        "    \"1c,20181019,12:00 pm,page_view,andorid,SG\"\n",
        "]\n",
        "outputs_prefix = 'outputs/part'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBvJXGyPhiIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "a32c00c8-a206-4ae7-b95d-1d00aba3d3fa"
      },
      "source": [
        "session_gap =  (60 * 60) - 60 \n",
        "\n",
        "print(\"removing old files\")\n",
        "run('rm -rf {}*'.format(\"outputs/\"))\n",
        "\n",
        "# class to prepare data before processing\n",
        "# add preprocessed session id field & formatted timestamp field from date & time\n",
        "class DataPrep(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        pcol = element.split(\",\")\n",
        "        ts_concat = pcol[1] + \" \" + pcol[2]\n",
        "        session_id_ = 's' + pcol[0] + \"_\" \n",
        "        session_ts = datetime.strptime(ts_concat, '%Y%m%d %I:%M %p').timestamp()\n",
        "        pcol.append(session_id_)\n",
        "        pcol.append(session_ts)\n",
        "        return [pcol]\n",
        "\n",
        "# class to add processed session id based on calculated window parameter \n",
        "class AddSessionId(beam.DoFn):\n",
        "  def process(self, element, window=beam.DoFn.WindowParam):\n",
        "    for x in element[1]:\n",
        "      x[6] += window.start.to_utc_datetime().strftime(\"%Y%m%d\") + \"_\" + window.start.to_utc_datetime().strftime(\"%H%M\")\n",
        "      logging.info(\" Event >> %s %s %s with start window=%s\"\\\n",
        "                   , x[0], x[3], x[7], window.start.to_utc_datetime())\n",
        "      yield x[:-1]\n",
        "\n",
        "# class to export file by session id\n",
        "class ExportFileBySession(beam.DoFn):\n",
        "    def __init__(self):\n",
        "        self.outdir = 'outputs/out-s1h_'\n",
        "    def process(self, element):\n",
        "        from apache_beam.io.filesystems import FileSystems \n",
        "        print(\"writing file: \" + element[0])\n",
        "        exp_file_name = self.outdir+element[0]+'.csv'\n",
        "        writer = FileSystems.create(exp_file_name, 'text/plain')\n",
        "        session_str = \"\"\n",
        "        for row in element[1]:\n",
        "          session_str += ','.join(row) + '\\n'\n",
        "        writer.write(session_str.encode())\n",
        "        writer.close()\n",
        "\n",
        "# apache beam processing starts here with defining pipeline object with options\n",
        "ptemp = beam.Pipeline(options=pipeline_options)\n",
        "\n",
        "# start processing files through series of PCollection processing (similar to spark)\n",
        "pline = (\n",
        "    ptemp\n",
        "    | 'loaddata' >> beam.Create(input_data) # can include variety of input using beam.io (batch/realtime)\n",
        "    | 'prepdata' >> beam.ParDo(DataPrep())\n",
        "    | 'addts' >> beam.Map(lambda elem: beam.window.TimestampedValue(elem, elem[7]))\n",
        "    | 'addkey' >> beam.Map(lambda row: (row[0], row))\n",
        "    | 'windowtimer' >> beam.WindowInto(beam.window.Sessions(session_gap)\\\n",
        "                                       , timestamp_combiner=beam.window.TimestampCombiner.OUTPUT_AT_EOW)\n",
        "    | 'groupbydevice' >> beam.GroupByKey()\n",
        "    | 'addsession' >> beam.ParDo(AddSessionId())\n",
        "    | 'prepexport' >> beam.Map(lambda row: (row[6], row))\n",
        "    | 'groupbysessionid' >> beam.GroupByKey()\n",
        ")\n",
        "\n",
        "# PCollection - just to print on console\n",
        "(\n",
        "    pline\n",
        "    | 'print' >> beam.Map(lambda element: print(element))\n",
        ")\n",
        "\n",
        "# PCollection - to export file by generated session id\n",
        "(\n",
        "    pline\n",
        "    | 'exportsession' >> beam.ParDo(ExportFileBySession())\n",
        ")\n",
        "\n",
        "result = ptemp.run()\n",
        "result.wait_until_finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removing old files\n",
            ">> rm -rf outputs/*\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:==================== <function annotate_downstream_side_inputs at 0x7fb695a33ae8> ====================\n",
            "INFO:root:==================== <function fix_side_input_pcoll_coders at 0x7fb695a33bf8> ====================\n",
            "INFO:root:==================== <function lift_combiners at 0x7fb695a33c80> ====================\n",
            "INFO:root:==================== <function expand_sdf at 0x7fb695a33d08> ====================\n",
            "INFO:root:==================== <function expand_gbk at 0x7fb695a33d90> ====================\n",
            "INFO:root:==================== <function sink_flattens at 0x7fb695a33ea0> ====================\n",
            "INFO:root:==================== <function greedily_fuse at 0x7fb695a33f28> ====================\n",
            "INFO:root:==================== <function read_to_impulse at 0x7fb695a44048> ====================\n",
            "INFO:root:==================== <function impulse_to_input at 0x7fb695a440d0> ====================\n",
            "INFO:root:==================== <function inject_timer_pcollections at 0x7fb695a44268> ====================\n",
            "INFO:root:==================== <function sort_stages at 0x7fb695a442f0> ====================\n",
            "INFO:root:==================== <function window_pcollection_coders at 0x7fb695a44378> ====================\n",
            "INFO:root:Running ((((ref_AppliedPTransform_loaddata/Impulse_3)+(ref_AppliedPTransform_loaddata/FlatMap(<lambda at core.py:2468>)_4))+(ref_AppliedPTransform_loaddata/MaybeReshuffle/Reshuffle/AddRandomKeys_7))+(ref_AppliedPTransform_loaddata/MaybeReshuffle/Reshuffle/ReshufflePerKey/Map(reify_timestamps)_9))+(loaddata/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Write)\n",
            "INFO:root:Running ((((((((loaddata/MaybeReshuffle/Reshuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_loaddata/MaybeReshuffle/Reshuffle/ReshufflePerKey/FlatMap(restore_timestamps)_14))+(ref_AppliedPTransform_loaddata/MaybeReshuffle/Reshuffle/RemoveRandomKeys_15))+(ref_AppliedPTransform_loaddata/Map(decode)_16))+(ref_AppliedPTransform_prepdata_17))+(ref_AppliedPTransform_addts_18))+(ref_AppliedPTransform_addkey_19))+(ref_AppliedPTransform_windowtimer_20))+(groupbydevice/Write)\n",
            "INFO:root:Running (((groupbydevice/Read)+(ref_AppliedPTransform_addsession_25))+(ref_AppliedPTransform_prepexport_26))+(groupbysessionid/Write)\n",
            "INFO:root: Event >> 1b click 1539943200.0 with start window=2018-10-19 10:00:00\n",
            "INFO:root: Event >> 1b install 1539943260.0 with start window=2018-10-19 10:00:00\n",
            "INFO:root: Event >> 1b page_view 1539946800.0 with start window=2018-10-19 11:00:00\n",
            "INFO:root: Event >> 1b order 1539947400.0 with start window=2018-10-19 11:00:00\n",
            "INFO:root: Event >> 1c page_view 1539943320.0 with start window=2018-10-19 10:02:00\n",
            "INFO:root: Event >> 1c order 1539943380.0 with start window=2018-10-19 10:02:00\n",
            "INFO:root: Event >> 1c page_view 1539950400.0 with start window=2018-10-19 12:00:00\n",
            "INFO:root:Running ((groupbysessionid/Read)+(ref_AppliedPTransform_print_31))+(ref_AppliedPTransform_exportsession_32)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('s1b_20181019_1000', [['1b', '20181019', '10:00 am', 'click', 'ios', 'SG', 's1b_20181019_1000'], ['1b', '20181019', '10:01 am', 'install', 'ios', 'SG', 's1b_20181019_1000']])\n",
            "writing file: s1b_20181019_1000\n",
            "('s1b_20181019_1100', [['1b', '20181019', '11:00 am', 'page_view', 'ios', 'SG', 's1b_20181019_1100'], ['1b', '20181019', '11:10 am', 'order', 'ios', 'SG', 's1b_20181019_1100']])\n",
            "writing file: s1b_20181019_1100\n",
            "('s1c_20181019_1002', [['1c', '20181019', '10:02 am', 'page_view', 'android', 'SG', 's1c_20181019_1002'], ['1c', '20181019', '10:03 am', 'order', 'android', 'SG', 's1c_20181019_1002']])\n",
            "writing file: s1c_20181019_1002\n",
            "('s1c_20181019_1200', [['1c', '20181019', '12:00 pm', 'page_view', 'andorid', 'SG', 's1c_20181019_1200']])\n",
            "writing file: s1c_20181019_1200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DONE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKguoqdzhlZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "424a807e-d959-4dfa-f793-49471645d3b2"
      },
      "source": [
        "run('head -n 20 {}*'.format(\"outputs/\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> head -n 20 outputs/*\n",
            "==> outputs/out-s1b_20181019_1000.csv <==\n",
            "1b,20181019,10:00 am,click,ios,SG,s1b_20181019_1000\n",
            "1b,20181019,10:01 am,install,ios,SG,s1b_20181019_1000\n",
            "\n",
            "==> outputs/out-s1b_20181019_1100.csv <==\n",
            "1b,20181019,11:00 am,page_view,ios,SG,s1b_20181019_1100\n",
            "1b,20181019,11:10 am,order,ios,SG,s1b_20181019_1100\n",
            "\n",
            "==> outputs/out-s1c_20181019_1002.csv <==\n",
            "1c,20181019,10:02 am,page_view,android,SG,s1c_20181019_1002\n",
            "1c,20181019,10:03 am,order,android,SG,s1c_20181019_1002\n",
            "\n",
            "==> outputs/out-s1c_20181019_1200.csv <==\n",
            "1c,20181019,12:00 pm,page_view,andorid,SG,s1c_20181019_1200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4_XgxZkGDR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}